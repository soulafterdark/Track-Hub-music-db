{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽµ Musical Tracks Database â€“ Core Build\n",
        "### SQLite project for track/artist/album data ingestion\n",
        "**Phase 1**: Core setup â€” imports, directories, schema, data loader  \n",
        "Author: Gabe M Chavez  \n"
      ],
      "metadata": {
        "id": "pJV27C4fn2sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 â€” Setup & Imports"
      ],
      "metadata": {
        "id": "_NNTeR_IouXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Project: Musical Tracks Database (Portfolio Version)\n",
        "# Author: Gabe M Chavez\n",
        "# Purpose: Build a clean, linear SQLite database from CSV/JSON\n",
        "#          inspired by the PY4E Tracks Assignment.\n",
        "# Phase: Core Build â€” Schema â†’ Ingest â†’ Assignment Joins\n",
        "# ==========================================================\n",
        "\n",
        "# Cell 1 â€” Setup & Imports\n",
        "from pathlib import Path\n",
        "import sqlite3\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Working directories (core-first build)\n",
        "WORKDIR = Path('/content/music_core')\n",
        "RAW_PATH = WORKDIR / 'raw'     # put your input CSV/JSON here\n",
        "DB_PATH = WORKDIR / 'musicdb.sqlite'\n",
        "\n",
        "# Ensure folders exist\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Working dir:\", WORKDIR)\n",
        "print(\"Raw data folder:\", RAW_PATH)\n",
        "print(\"DB path:\", DB_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCWNOsH4oyM-",
        "outputId": "662d0cc7-206b-4220-def2-e47198cb72b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working dir: /content/music_core\n",
            "Raw data folder: /content/music_core/raw\n",
            "DB path: /content/music_core/musicdb.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 â€” Helper Functions"
      ],
      "metadata": {
        "id": "U3H72iJVo8hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 2 â€” Helper Functions\n",
        "# Purpose: Basic database helpers for connecting, running SQL,\n",
        "#          and fetching results. Keeps code clean for later cells.\n",
        "# ==========================================================\n",
        "\n",
        "def connect_db(db_path=DB_PATH):\n",
        "    \"\"\"Create (or connect to) the SQLite database.\"\"\"\n",
        "    return sqlite3.connect(db_path)\n",
        "\n",
        "def run_sql(conn, sql, params=None):\n",
        "    \"\"\"Execute SQL (DDL/DML) with optional params and commit.\"\"\"\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sql, params or [])\n",
        "    conn.commit()\n",
        "\n",
        "def query_all(conn, sql, params=None):\n",
        "    \"\"\"Return list of rows from a SELECT query.\"\"\"\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sql, params or [])\n",
        "    return cur.fetchall()\n",
        "\n",
        "def query_one(conn, sql, params=None):\n",
        "    \"\"\"Return a single row or None.\"\"\"\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sql, params or [])\n",
        "    return cur.fetchone()\n",
        "\n",
        "print(\"Helper functions loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4PyiXxo9nH",
        "outputId": "bcb92eba-2350-4e9a-ca4d-b80e0099352d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 â€” Create Schema (DDL)\n"
      ],
      "metadata": {
        "id": "haoAZTgbpB4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 3 â€” Create Schema (DDL)\n",
        "# Purpose: Define normalized tables for Artist, Album, Genre, Track.\n",
        "# Notes:\n",
        "# - Uses UNIQUE constraints to avoid duplicates.\n",
        "# - Enables FOREIGN KEY integrity.\n",
        "# ==========================================================\n",
        "\n",
        "SCHEMA_DDL = [\n",
        "    \"\"\"\n",
        "    PRAGMA foreign_keys = ON;\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS Artist(\n",
        "        id   INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        name TEXT UNIQUE NOT NULL\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS Album(\n",
        "        id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        title     TEXT NOT NULL,\n",
        "        artist_id INTEGER NOT NULL,\n",
        "        year      INTEGER,\n",
        "        FOREIGN KEY(artist_id) REFERENCES Artist(id),\n",
        "        UNIQUE(title, artist_id)\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS Genre(\n",
        "        id   INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        name TEXT UNIQUE\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS Track(\n",
        "        id           INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        title        TEXT NOT NULL,\n",
        "        album_id     INTEGER NOT NULL,\n",
        "        genre_id     INTEGER,\n",
        "        milliseconds INTEGER,\n",
        "        rating       INTEGER,\n",
        "        play_count   INTEGER,\n",
        "        FOREIGN KEY(album_id) REFERENCES Album(id),\n",
        "        FOREIGN KEY(genre_id) REFERENCES Genre(id),\n",
        "        UNIQUE(title, album_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "def create_schema(conn):\n",
        "    cur = conn.cursor()\n",
        "    for stmt in SCHEMA_DDL:\n",
        "        cur.execute(stmt)\n",
        "    conn.commit()\n",
        "\n",
        "# Execute once to build schema\n",
        "conn = connect_db(DB_PATH)\n",
        "create_schema(conn)\n",
        "conn.close()\n",
        "print(\"Schema created (Artist, Album, Genre, Track). Foreign keys enabled.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOVO0Z3YpN1J",
        "outputId": "a15c0bc6-1203-4873-da7f-6ee77528907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema created (Artist, Album, Genre, Track). Foreign keys enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 â€” Data Cleaning Utils"
      ],
      "metadata": {
        "id": "MvN81I0hpW_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 4 â€” Data Cleaning Utils\n",
        "# Purpose: Normalize raw strings before upserts (trim, collapse\n",
        "#          whitespace, standardize casing). Keeps ETL consistent.\n",
        "# ==========================================================\n",
        "import re\n",
        "\n",
        "_ws_re = re.compile(r\"\\s+\")\n",
        "\n",
        "def _norm_base(s):\n",
        "    \"\"\"Trim and collapse inner whitespace. Return None if empty.\"\"\"\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = str(s).strip()\n",
        "    s = _ws_re.sub(\" \", s)\n",
        "    return s if s else None\n",
        "\n",
        "def clean_artist(name: str):\n",
        "    \"\"\"Normalize artist names; keep original case where reasonable.\"\"\"\n",
        "    name = _norm_base(name)\n",
        "    return name\n",
        "\n",
        "def clean_album(title: str):\n",
        "    \"\"\"Normalize album titles; preserve case.\"\"\"\n",
        "    title = _norm_base(title)\n",
        "    return title\n",
        "\n",
        "def clean_title(title: str):\n",
        "    \"\"\"Normalize track titles; preserve case.\"\"\"\n",
        "    title = _norm_base(title)\n",
        "    return title\n",
        "\n",
        "def clean_genre(name: str):\n",
        "    \"\"\"Normalize genre names to Title Case (e.g., 'deep house' -> 'Deep House').\"\"\"\n",
        "    name = _norm_base(name)\n",
        "    if name is None:\n",
        "        return None\n",
        "    return name.title()\n",
        "\n",
        "print(\"Cleaning helpers ready: clean_artist, clean_album, clean_title, clean_genre\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psihpq0cpX77",
        "outputId": "f52cce66-f578-418b-ae98-8a03b4d48db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning helpers ready: clean_artist, clean_album, clean_title, clean_genre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 â€” Loader (CSV/JSON) + Upserts"
      ],
      "metadata": {
        "id": "y7EtDwMapcA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 5 â€” Loader (CSV/JSON) + Upserts\n",
        "# Purpose: Read rows from CSV/JSON and upsert into Artist, Album,\n",
        "#          Genre, Track with idempotent operations.\n",
        "# Notes:\n",
        "# - Accepts flexible column names (e.g., title/track, milliseconds/length).\n",
        "# - Uses INSERT OR IGNORE, then SELECT id to fetch PKs.\n",
        "# ==========================================================\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Upserts ----------\n",
        "\n",
        "def upsert_artist(conn, name: str) -> int:\n",
        "    name = clean_artist(name)\n",
        "    if not name:\n",
        "        raise ValueError(\"Artist name is required\")\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"INSERT OR IGNORE INTO Artist(name) VALUES (?)\", (name,))\n",
        "    cur.execute(\"SELECT id FROM Artist WHERE name = ?\", (name,))\n",
        "    return cur.fetchone()[0]\n",
        "\n",
        "def upsert_album(conn, title: str, artist_name: str, year: int | None = None) -> int:\n",
        "    title = clean_album(title)\n",
        "    if not title:\n",
        "        raise ValueError(\"Album title is required\")\n",
        "    artist_id = upsert_artist(conn, artist_name)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"INSERT OR IGNORE INTO Album(title, artist_id, year) VALUES (?, ?, ?)\",\n",
        "        (title, artist_id, year),\n",
        "    )\n",
        "    cur.execute(\"SELECT id FROM Album WHERE title = ? AND artist_id = ?\", (title, artist_id))\n",
        "    return cur.fetchone()[0]\n",
        "\n",
        "def upsert_genre(conn, name: str | None) -> int | None:\n",
        "    name = clean_genre(name)\n",
        "    if not name:\n",
        "        return None\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"INSERT OR IGNORE INTO Genre(name) VALUES (?)\", (name,))\n",
        "    cur.execute(\"SELECT id FROM Genre WHERE name = ?\", (name,))\n",
        "    row = cur.fetchone()\n",
        "    return row[0] if row else None\n",
        "\n",
        "def upsert_track(\n",
        "    conn,\n",
        "    title: str,\n",
        "    album_title: str,\n",
        "    artist_name: str,\n",
        "    genre_name: str | None = None,\n",
        "    milliseconds: int | None = None,\n",
        "    rating: int | None = None,\n",
        "    play_count: int | None = None,\n",
        "    year: int | None = None,\n",
        ") -> int:\n",
        "    title = clean_title(title)\n",
        "    if not title:\n",
        "        raise ValueError(\"Track title is required\")\n",
        "\n",
        "    album_id = upsert_album(conn, album_title, artist_name, year)\n",
        "    genre_id = upsert_genre(conn, genre_name)\n",
        "\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT OR IGNORE INTO Track(title, album_id, genre_id, milliseconds, rating, play_count)\n",
        "        VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (title, album_id, genre_id, milliseconds, rating, play_count),\n",
        "    )\n",
        "    # Uniqueness is (title, album_id)\n",
        "    cur.execute(\"SELECT id FROM Track WHERE title = ? AND album_id = ?\", (title, album_id))\n",
        "    return cur.fetchone()[0]\n",
        "\n",
        "# ---------- Row parsing helpers ----------\n",
        "\n",
        "def _get_any(d: dict, *names, default=None):\n",
        "    \"\"\"Return the first present non-empty field from candidate names (case-insensitive).\"\"\"\n",
        "    # build case-insensitive map\n",
        "    lower_map = {str(k).lower(): v for k, v in d.items()}\n",
        "    for n in names:\n",
        "        if n is None:\n",
        "            continue\n",
        "        v = lower_map.get(str(n).lower())\n",
        "        if v is not None and str(v).strip() != \"\":\n",
        "            return v\n",
        "    return default\n",
        "\n",
        "def _to_int_or_none(x):\n",
        "    try:\n",
        "        if x is None or str(x).strip() == \"\":\n",
        "            return None\n",
        "        return int(float(x))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Accept common field names from varied exports\n",
        "FIELD_MAP = {\n",
        "    \"title\": (\"title\", \"track\", \"name\", \"track_name\"),\n",
        "    \"artist\": (\"artist\", \"artist_name\"),\n",
        "    \"album\": (\"album\", \"album_title\"),\n",
        "    \"genre\": (\"genre\",),\n",
        "    \"milliseconds\": (\"milliseconds\", \"duration_ms\", \"length\", \"time_ms\"),\n",
        "    \"rating\": (\"rating\", \"stars\"),\n",
        "    \"play_count\": (\"play_count\", \"plays\", \"playcount\"),\n",
        "    \"year\": (\"year\", \"release_year\"),\n",
        "}\n",
        "\n",
        "# ---------- CSV / JSON readers ----------\n",
        "\n",
        "def load_rows_from_csv(path: Path):\n",
        "    \"\"\"Yield dictionaries from CSV with flexible headers.\"\"\"\n",
        "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            yield row\n",
        "\n",
        "def load_rows_from_json(path: Path):\n",
        "    \"\"\"Yield dictionaries from JSON. Supports list[...] or dict{items:[...] }.\"\"\"\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, list):\n",
        "        for row in data:\n",
        "            yield row\n",
        "    elif isinstance(data, dict):\n",
        "        # try common container keys\n",
        "        for key in (\"items\", \"tracks\", \"rows\", \"data\"):\n",
        "            if key in data and isinstance(data[key], list):\n",
        "                for row in data[key]:\n",
        "                    yield row\n",
        "                return\n",
        "        # fallback: yield the dict itself if it looks like a single row\n",
        "        yield data\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported JSON structure\")\n",
        "\n",
        "def row_to_track_kwargs(row: dict) -> dict:\n",
        "    \"\"\"Map a raw row to kwargs for upsert_track with cleaning and type conversion.\"\"\"\n",
        "    title = _get_any(row, *FIELD_MAP[\"title\"])\n",
        "    artist = _get_any(row, *FIELD_MAP[\"artist\"])\n",
        "    album = _get_any(row, *FIELD_MAP[\"album\"])\n",
        "    genre = _get_any(row, *FIELD_MAP[\"genre\"])\n",
        "    milliseconds = _to_int_or_none(_get_any(row, *FIELD_MAP[\"milliseconds\"]))\n",
        "    rating = _to_int_or_none(_get_any(row, *FIELD_MAP[\"rating\"]))\n",
        "    play_count = _to_int_or_none(_get_any(row, *FIELD_MAP[\"play_count\"]))\n",
        "    year = _to_int_or_none(_get_any(row, *FIELD_MAP[\"year\"]))\n",
        "\n",
        "    return dict(\n",
        "        title=title,\n",
        "        album_title=album,\n",
        "        artist_name=artist,\n",
        "        genre_name=genre,\n",
        "        milliseconds=milliseconds,\n",
        "        rating=rating,\n",
        "        play_count=play_count,\n",
        "        year=year,\n",
        "    )\n",
        "\n",
        "print(\"Loaders and upserts ready (CSV/JSON, flexible fields).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMZeSwvwpn4M",
        "outputId": "14657be6-b941-4789-fd65-e00c8ac1d6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders and upserts ready (CSV/JSON, flexible fields).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6 â€” ETL Pipeline (ingest)"
      ],
      "metadata": {
        "id": "xf-PxprUpy9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 6 â€” ETL Pipeline (ingest)\n",
        "# Purpose: Read a CSV/JSON file, transform rows -> upserts, and\n",
        "#          load into the DB. Idempotent & safe to re-run.\n",
        "# ==========================================================\n",
        "from typing import Iterable\n",
        "\n",
        "def _detect_loader(path: Path):\n",
        "    suf = path.suffix.lower()\n",
        "    if suf == \".csv\":\n",
        "        return load_rows_from_csv\n",
        "    if suf in {\".json\", \".jsn\"}:\n",
        "        return load_rows_from_json\n",
        "    raise ValueError(f\"Unsupported file type: {suf}\")\n",
        "\n",
        "def ingest_file(path: Path, batch_size: int = 500) -> dict:\n",
        "    \"\"\"\n",
        "    Ingest a single CSV/JSON file into the database.\n",
        "    Returns stats dict: {'rows': int, 'inserted': int, 'skipped': int, 'errors': int}\n",
        "    \"\"\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "    loader = _detect_loader(path)\n",
        "\n",
        "    stats = {\"rows\": 0, \"inserted\": 0, \"skipped\": 0, \"errors\": 0}\n",
        "    conn = connect_db(DB_PATH)\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        for row in loader(path):\n",
        "            stats[\"rows\"] += 1\n",
        "            try:\n",
        "                kwargs = row_to_track_kwargs(row)\n",
        "                # minimal required fields\n",
        "                if not kwargs[\"title\"] or not kwargs[\"album_title\"] or not kwargs[\"artist_name\"]:\n",
        "                    stats[\"skipped\"] += 1\n",
        "                    continue\n",
        "                _ = upsert_track(conn, **kwargs)\n",
        "                stats[\"inserted\"] += 1\n",
        "            except Exception as e:\n",
        "                stats[\"errors\"] += 1\n",
        "                # Lightweight debug: show every 200th error to avoid noise\n",
        "                if stats[\"errors\"] <= 5 or stats[\"errors\"] % 200 == 0:\n",
        "                    print(\"Row error:\", e)\n",
        "\n",
        "            # Commit in batches\n",
        "            if stats[\"rows\"] % batch_size == 0:\n",
        "                conn.commit()\n",
        "                print(f\" ... processed {stats['rows']} rows\")\n",
        "\n",
        "        conn.commit()\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    print(f\"Ingest complete: {stats}\")\n",
        "    return stats\n",
        "\n",
        "def ingest_all_in_raw() -> dict:\n",
        "    \"\"\"\n",
        "    Ingest all CSV/JSON files found in RAW_PATH.\n",
        "    Aggregates stats across files.\n",
        "    \"\"\"\n",
        "    totals = {\"rows\": 0, \"inserted\": 0, \"skipped\": 0, \"errors\": 0, \"files\": 0}\n",
        "    files = sorted([p for p in RAW_PATH.iterdir() if p.suffix.lower() in {\".csv\", \".json\", \".jsn\"}])\n",
        "    if not files:\n",
        "        print(f\"No CSV/JSON files found in {RAW_PATH}\")\n",
        "        return totals\n",
        "    for f in files:\n",
        "        print(f\"Ingesting: {f.name}\")\n",
        "        s = ingest_file(f)\n",
        "        for k in (\"rows\", \"inserted\", \"skipped\", \"errors\"):\n",
        "            totals[k] += s[k]\n",
        "        totals[\"files\"] += 1\n",
        "    print(f\"All ingests complete: {totals}\")\n",
        "    return totals\n",
        "\n",
        "print(\"ETL ready: ingest_file(path), ingest_all_in_raw()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RNIbFAdp4Mm",
        "outputId": "f45d32a6-1622-43f4-f9a7-8fb8dfa15d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETL ready: ingest_file(path), ingest_all_in_raw()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7 â€” Sanity Checks"
      ],
      "metadata": {
        "id": "4cTw4GA2qSv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 7 â€” Sanity Checks\n",
        "# Purpose: Quick integrity & content checks after ingestion.\n",
        "# - Table row counts\n",
        "# - SQLite integrity + foreign key checks\n",
        "# - Data quality: tracks missing genre\n",
        "# - Preview sample rows from each table\n",
        "# ==========================================================\n",
        "\n",
        "def _print_counts(conn):\n",
        "    tables = [\"Artist\", \"Album\", \"Genre\", \"Track\"]\n",
        "    for t in tables:\n",
        "        row = query_one(conn, f\"SELECT COUNT(*) FROM {t}\")\n",
        "        print(f\"{t:>6}: {row[0] if row else 0}\")\n",
        "\n",
        "def _print_sample(conn, table, limit=5):\n",
        "    print(f\"\\nSample from {table} (up to {limit} rows):\")\n",
        "    rows = query_all(conn, f\"SELECT * FROM {table} LIMIT {limit}\")\n",
        "    for r in rows:\n",
        "        print(\"  \", r)\n",
        "\n",
        "def sanity_report():\n",
        "    conn = connect_db(DB_PATH)\n",
        "    try:\n",
        "        print(\"== Table counts ==\")\n",
        "        _print_counts(conn)\n",
        "\n",
        "        print(\"\\n== SQLite integrity checks ==\")\n",
        "        print(\"PRAGMA integrity_check =>\", query_one(conn, \"PRAGMA integrity_check;\")[0])\n",
        "        fk_rows = query_all(conn, \"PRAGMA foreign_key_check;\")\n",
        "        if fk_rows:\n",
        "            print(\"Foreign key issues:\")\n",
        "            for r in fk_rows[:10]:\n",
        "                print(\"  \", r)\n",
        "            if len(fk_rows) > 10:\n",
        "                print(f\"  ... and {len(fk_rows) - 10} more\")\n",
        "        else:\n",
        "            print(\"Foreign key check => OK\")\n",
        "\n",
        "        print(\"\\n== Data quality checks ==\")\n",
        "        # Tracks without a genre are allowed, but weâ€™ll report how many:\n",
        "        missing_genre = query_one(conn, \"SELECT COUNT(*) FROM Track WHERE genre_id IS NULL\")[0]\n",
        "        print(f\"Tracks missing genre: {missing_genre}\")\n",
        "\n",
        "        # Duplicate (title, album) attempts (should be prevented by UNIQUE):\n",
        "        dupes = query_one(conn, \"\"\"\n",
        "            SELECT COUNT(*) FROM (\n",
        "                SELECT title, album_id, COUNT(*) c\n",
        "                FROM Track\n",
        "                GROUP BY title, album_id\n",
        "                HAVING c > 1\n",
        "            )\n",
        "        \"\"\")[0]\n",
        "        print(f\"Duplicate (title, album) groups: {dupes}\")\n",
        "\n",
        "        # Orphan checks (should be zero due to FK constraints):\n",
        "        orphan_albums = query_one(conn, \"\"\"\n",
        "            SELECT COUNT(*) FROM Album a\n",
        "            WHERE NOT EXISTS (SELECT 1 FROM Artist ar WHERE ar.id = a.artist_id)\n",
        "        \"\"\")[0]\n",
        "        print(f\"Albums without a valid artist (orphans): {orphan_albums}\")\n",
        "\n",
        "        print(\"\\n== Samples ==\")\n",
        "        _print_sample(conn, \"Artist\")\n",
        "        _print_sample(conn, \"Album\")\n",
        "        _print_sample(conn, \"Genre\")\n",
        "        _print_sample(conn, \"Track\")\n",
        "\n",
        "    finally:\n",
        "        conn.close()\n",
        "    print(\"\\nSanity report complete.\")\n",
        "\n",
        "print(\"Sanity checks ready. Call sanity_report() after ingest.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpY0AG3bqTsq",
        "outputId": "c4c2f62b-345b-4d33-d19e-291344dd8743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks ready. Call sanity_report() after ingest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 â€” Assignment Joins (Top 3, etc.)"
      ],
      "metadata": {
        "id": "r9gZlO0Rqmwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 8 â€” Assignment Joins (Top 3, etc.)\n",
        "# Purpose: Run the canonical joined SELECTs for the assignment.\n",
        "# Outputs:\n",
        "#  - Top 3 joined rows: Track, Artist, Album, Genre (alphabetical)\n",
        "#  - Optional: a few handy summaries (counts)\n",
        "# ==========================================================\n",
        "\n",
        "def run_assignment_queries(limit=3):\n",
        "    conn = connect_db(DB_PATH)\n",
        "    try:\n",
        "        print(\"== Top joined results ==\")\n",
        "        rows = query_all(conn, f\"\"\"\n",
        "            SELECT Track.title, Artist.name, Album.title, COALESCE(Genre.name, 'Unknown')\n",
        "            FROM Track\n",
        "            JOIN Album ON Track.album_id = Album.id\n",
        "            JOIN Artist ON Album.artist_id = Artist.id\n",
        "            LEFT JOIN Genre ON Track.genre_id = Genre.id\n",
        "            ORDER BY Artist.name, Album.title, Track.title\n",
        "            LIMIT {int(limit)};\n",
        "        \"\"\")\n",
        "        for r in rows:\n",
        "            print(tuple(r))\n",
        "\n",
        "        # Optional small summaries (helpful for sanity and grading)\n",
        "        print(\"\\n== Summary counts ==\")\n",
        "        print(\"Artists:\", query_one(conn, \"SELECT COUNT(*) FROM Artist\")[0])\n",
        "        print(\"Albums :\", query_one(conn, \"SELECT COUNT(*) FROM Album\")[0])\n",
        "        print(\"Genres :\", query_one(conn, \"SELECT COUNT(*) FROM Genre\")[0])\n",
        "        print(\"Tracks :\", query_one(conn, \"SELECT COUNT(*) FROM Track\")[0])\n",
        "\n",
        "    finally:\n",
        "        conn.close()\n",
        "    print(\"\\nAssignment queries complete.\")\n",
        "\n",
        "print(\"Assignment queries ready. Call run_assignment_queries().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMSGECFlqni0",
        "outputId": "def930e1-ce14-4472-e37b-2656adf84320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment queries ready. Call run_assignment_queries().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9 â€” Analytics & Insights"
      ],
      "metadata": {
        "id": "WOeEYqDbrFBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 9 â€” Analytics & Insights\n",
        "# Purpose: Portfolio-friendly summaries you can show in README.\n",
        "# Outputs (printed):\n",
        "#  - Top artists by number of tracks\n",
        "#  - Top genres by tracks\n",
        "#  - Longest tracks (by milliseconds)\n",
        "#  - Albums with most tracks\n",
        "#  - Data quality: tracks missing genre, duplicate titles per artist\n",
        "# ==========================================================\n",
        "\n",
        "def _print_rows(title, rows, limit=10):\n",
        "    print(f\"\\n== {title} (top {limit}) ==\")\n",
        "    for r in rows[:limit]:\n",
        "        print(\"  \", tuple(r))\n",
        "\n",
        "def analytics_report(top_n=10):\n",
        "    conn = connect_db(DB_PATH)\n",
        "    try:\n",
        "        # Top artists by track count\n",
        "        artists = query_all(conn, \"\"\"\n",
        "            SELECT ar.name, COUNT(t.id) AS track_count\n",
        "            FROM Artist ar\n",
        "            JOIN Album al ON al.artist_id = ar.id\n",
        "            JOIN Track t  ON t.album_id = al.id\n",
        "            GROUP BY ar.id\n",
        "            ORDER BY track_count DESC, ar.name ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _print_rows(\"Top Artists by Track Count\", artists, limit=top_n)\n",
        "\n",
        "        # Top genres by track count\n",
        "        genres = query_all(conn, \"\"\"\n",
        "            SELECT COALESCE(g.name, 'Unknown') AS genre, COUNT(t.id) AS track_count\n",
        "            FROM Track t\n",
        "            LEFT JOIN Genre g ON t.genre_id = g.id\n",
        "            GROUP BY genre\n",
        "            ORDER BY track_count DESC, genre ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _print_rows(\"Top Genres by Track Count\", genres, limit=top_n)\n",
        "\n",
        "        # Longest tracks\n",
        "        longest = query_all(conn, \"\"\"\n",
        "            SELECT t.title, ar.name, al.title, t.milliseconds\n",
        "            FROM Track t\n",
        "            JOIN Album al ON t.album_id = al.id\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            WHERE t.milliseconds IS NOT NULL\n",
        "            ORDER BY t.milliseconds DESC, t.title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _print_rows(\"Longest Tracks (ms)\", longest, limit=top_n)\n",
        "\n",
        "        # Albums with most tracks\n",
        "        albums = query_all(conn, \"\"\"\n",
        "            SELECT ar.name, al.title, COUNT(t.id) AS track_count\n",
        "            FROM Album al\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            JOIN Track t   ON t.album_id = al.id\n",
        "            GROUP BY al.id\n",
        "            ORDER BY track_count DESC, ar.name ASC, al.title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _print_rows(\"Albums with Most Tracks\", albums, limit=top_n)\n",
        "\n",
        "        # Data quality checks\n",
        "        print(\"\\n== Data Quality ==\")\n",
        "        missing_genre = query_one(conn, \"SELECT COUNT(*) FROM Track WHERE genre_id IS NULL\")[0]\n",
        "        print(\"Tracks missing genre:\", missing_genre)\n",
        "\n",
        "        dup_title_artist = query_all(conn, \"\"\"\n",
        "            SELECT ar.name, t.title, COUNT(*) AS c\n",
        "            FROM Track t\n",
        "            JOIN Album al ON t.album_id = al.id\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            GROUP BY ar.name, t.title\n",
        "            HAVING c > 1\n",
        "            ORDER BY c DESC, ar.name ASC, t.title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _print_rows(\"Duplicate Titles per Artist\", dup_title_artist, limit=top_n)\n",
        "\n",
        "    finally:\n",
        "        conn.close()\n",
        "    print(\"\\nAnalytics report complete. Consider capturing outputs for README.\")\n",
        "\n",
        "print(\"Analytics ready. Call analytics_report(top_n=10).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssUWlrpxrMfh",
        "outputId": "ba043b05-898a-4c8e-f46f-51f5d2a1384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytics ready. Call analytics_report(top_n=10).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10 â€” Export Tools"
      ],
      "metadata": {
        "id": "rRJ5MYfXrW7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 10 â€” Export Tools\n",
        "# Purpose: Save portfolio-ready CSV summaries to /exports.\n",
        "# Exports:\n",
        "#  - top_artists.csv\n",
        "#  - top_genres.csv\n",
        "#  - longest_tracks.csv\n",
        "#  - albums_most_tracks.csv\n",
        "#  - tracks_missing_genre.csv\n",
        "#  - duplicate_titles_per_artist.csv\n",
        "# ==========================================================\n",
        "from pathlib import Path\n",
        "\n",
        "EXPORTS_PATH = WORKDIR / \"exports\"\n",
        "EXPORTS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _write_csv(path: Path, header: list[str], rows: list[tuple]):\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(header)\n",
        "        for r in rows:\n",
        "            w.writerow(list(r))\n",
        "\n",
        "def export_summaries(top_n=100):\n",
        "    conn = connect_db(DB_PATH)\n",
        "    try:\n",
        "        # Top artists by track count\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            SELECT ar.name, COUNT(t.id) AS track_count\n",
        "            FROM Artist ar\n",
        "            JOIN Album al ON al.artist_id = ar.id\n",
        "            JOIN Track t  ON t.album_id = al.id\n",
        "            GROUP BY ar.id\n",
        "            ORDER BY track_count DESC, ar.name ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _write_csv(EXPORTS_PATH / \"top_artists.csv\",\n",
        "                   [\"artist\", \"track_count\"], rows)\n",
        "\n",
        "        # Top genres by track count\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            SELECT COALESCE(g.name, 'Unknown') AS genre, COUNT(t.id) AS track_count\n",
        "            FROM Track t\n",
        "            LEFT JOIN Genre g ON t.genre_id = g.id\n",
        "            GROUP BY genre\n",
        "            ORDER BY track_count DESC, genre ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _write_csv(EXPORTS_PATH / \"top_genres.csv\",\n",
        "                   [\"genre\", \"track_count\"], rows)\n",
        "\n",
        "        # Longest tracks\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            SELECT t.title, ar.name AS artist, al.title AS album, t.milliseconds\n",
        "            FROM Track t\n",
        "            JOIN Album al ON t.album_id = al.id\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            WHERE t.milliseconds IS NOT NULL\n",
        "            ORDER BY t.milliseconds DESC, t.title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _write_csv(EXPORTS_PATH / \"longest_tracks.csv\",\n",
        "                   [\"title\", \"artist\", \"album\", \"milliseconds\"], rows)\n",
        "\n",
        "        # Albums with most tracks\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            SELECT ar.name AS artist, al.title AS album, COUNT(t.id) AS track_count\n",
        "            FROM Album al\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            JOIN Track t   ON t.album_id = al.id\n",
        "            GROUP BY al.id\n",
        "            ORDER BY track_count DESC, artist ASC, album ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _write_csv(EXPORTS_PATH / \"albums_most_tracks.csv\",\n",
        "                   [\"artist\", \"album\", \"track_count\"], rows)\n",
        "\n",
        "        # Tracks missing genre\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            SELECT t.title, ar.name AS artist, al.title AS album\n",
        "            FROM Track t\n",
        "            JOIN Album al ON t.album_id = al.id\n",
        "            JOIN Artist ar ON al.artist_id = ar.id\n",
        "            WHERE t.genre_id IS NULL\n",
        "            ORDER BY artist ASC, album ASC, t.title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (max(top_n, 1000),))  # allow more here\n",
        "        _write_csv(EXPORTS_PATH / \"tracks_missing_genre.csv\",\n",
        "                   [\"title\", \"artist\", \"album\"], rows)\n",
        "\n",
        "        # Duplicate titles per artist (same title appearing >1 across albums)\n",
        "        rows = query_all(conn, \"\"\"\n",
        "            WITH title_counts AS (\n",
        "                SELECT ar.name AS artist, t.title, COUNT(*) AS c\n",
        "                FROM Track t\n",
        "                JOIN Album al ON t.album_id = al.id\n",
        "                JOIN Artist ar ON al.artist_id = ar.id\n",
        "                GROUP BY ar.name, t.title\n",
        "                HAVING c > 1\n",
        "            )\n",
        "            SELECT artist, title, c\n",
        "            FROM title_counts\n",
        "            ORDER BY c DESC, artist ASC, title ASC\n",
        "            LIMIT ?;\n",
        "        \"\"\", (top_n,))\n",
        "        _write_csv(EXPORTS_PATH / \"duplicate_titles_per_artist.csv\",\n",
        "                   [\"artist\", \"title\", \"count\"], rows)\n",
        "\n",
        "        print(\"Exports written to:\", EXPORTS_PATH)\n",
        "\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "print(\"Export tools ready. Call export_summaries(top_n=100).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_MOuRlrk3K",
        "outputId": "e19286b1-cdc7-40b5-faa1-cc98f8910830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export tools ready. Call export_summaries(top_n=100).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11 â€” README Generator"
      ],
      "metadata": {
        "id": "FlLiVBxlrs5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 11 â€” README Generator (Safe Version)\n",
        "# Purpose: Create a clean README.md without f-strings or triple-quote issues.\n",
        "# Output: /content/music_core/README.md\n",
        "# ==========================================================\n",
        "from datetime import datetime\n",
        "\n",
        "README_PATH = WORKDIR / \"README.md\"\n",
        "\n",
        "SCHEMA_ASCII = r\"\"\"\n",
        "Schema (SQLite)\n",
        "---------------\n",
        "Artist(id PK, name UNIQUE NOT NULL)\n",
        "Album(id PK, title NOT NULL, artist_id FK->Artist(id), year, UNIQUE(title, artist_id))\n",
        "Genre(id PK, name UNIQUE)\n",
        "Track(id PK, title NOT NULL, album_id FK->Album(id), genre_id FK->Genre(id),\n",
        "      milliseconds, rating, play_count, UNIQUE(title, album_id))\n",
        "\"\"\"\n",
        "\n",
        "def _fetch_counts(conn):\n",
        "    return {\n",
        "        \"artists\":  query_one(conn, \"SELECT COUNT(*) FROM Artist\")[0],\n",
        "        \"albums\":   query_one(conn, \"SELECT COUNT(*) FROM Album\")[0],\n",
        "        \"genres\":   query_one(conn, \"SELECT COUNT(*) FROM Genre\")[0],\n",
        "        \"tracks\":   query_one(conn, \"SELECT COUNT(*) FROM Track\")[0],\n",
        "        \"missing_genre\": query_one(conn, \"SELECT COUNT(*) FROM Track WHERE genre_id IS NULL\")[0],\n",
        "    }\n",
        "\n",
        "def _fetch_top_preview(conn, limit=5):\n",
        "    rows = query_all(conn, \"\"\"\n",
        "        SELECT t.title, ar.name, al.title, COALESCE(g.name, 'Unknown') AS genre\n",
        "        FROM Track t\n",
        "        JOIN Album al ON t.album_id = al.id\n",
        "        JOIN Artist ar ON al.artist_id = ar.id\n",
        "        LEFT JOIN Genre g ON t.genre_id = g.id\n",
        "        ORDER BY ar.name, al.title, t.title\n",
        "        LIMIT ?;\n",
        "    \"\"\", (int(limit),))\n",
        "    lines = [\"| Track | Artist | Album | Genre |\", \"|---|---|---|---|\"]\n",
        "    for r in rows:\n",
        "        lines.append(f\"| {r[0]} | {r[1]} | {r[2]} | {r[3]} |\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def generate_readme():\n",
        "    conn = connect_db(DB_PATH)\n",
        "    try:\n",
        "        counts = _fetch_counts(conn)\n",
        "        preview_table_md = _fetch_top_preview(conn)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    now = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
        "\n",
        "    # Build README text as a list of lines\n",
        "    lines = []\n",
        "    lines.append(\"# Musical Tracks Database â€“ Core Build\\n\")\n",
        "    lines.append(\"A clean, **linear** SQLite build for track/artist/album ingestion.\\n\")\n",
        "    lines.append(f\"- **Author**: Gabe M Chavez\")\n",
        "    lines.append(f\"- **Last Updated**: {now}\")\n",
        "    lines.append(f\"- **Tracks loaded**: {counts['tracks']}\\n\")\n",
        "    lines.append(\"---\\n\")\n",
        "    lines.append(\"## Quick Start (Colab)\")\n",
        "    lines.append(\"1. Run Cells 1-8 in order.\")\n",
        "    lines.append(f\"2. Put your input data into: `{RAW_PATH}`\")\n",
        "    lines.append(\"3. Ingest and check:\")\n",
        "    lines.append(\"    ```python\")\n",
        "    lines.append(\"    stats = ingest_all_in_raw()\")\n",
        "    lines.append(\"    sanity_report()\")\n",
        "    lines.append(\"    run_assignment_queries(limit=3)\")\n",
        "    lines.append(\"    ```\\n\")\n",
        "    lines.append(\"---\\n\")\n",
        "    lines.append(\"## Sample Joined Rows (Top 5)\\n\")\n",
        "    lines.append(preview_table_md + \"\\n\")\n",
        "    lines.append(\"---\\n\")\n",
        "    lines.append(\"## Schema\")\n",
        "    lines.append(\"```text\")\n",
        "    lines.append(SCHEMA_ASCII.strip())\n",
        "    lines.append(\"```\\n\")\n",
        "    lines.append(\"---\\n\")\n",
        "    lines.append(f\"## Project Structure\\n`{WORKDIR}`\\n\")\n",
        "    lines.append(\"- `musicdb.sqlite` â€” Final SQLite database\")\n",
        "    lines.append(\"- `raw/` â€” Input CSV/JSON files\")\n",
        "    lines.append(\"- `exports/` â€” CSV outputs from Cell 10\")\n",
        "    lines.append(\"- `README.md` â€” This file\\n\")\n",
        "\n",
        "    # Write to file\n",
        "    README_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "    print(\"README generated at:\", README_PATH)\n",
        "\n",
        "print(\"README generator ready. Call generate_readme().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKQK_w-AtFSd",
        "outputId": "c343ee13-1789-45eb-a971-41c8005ab892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README generator ready. Call generate_readme().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12 â€” Notebook Wrap-Up"
      ],
      "metadata": {
        "id": "-y7UiHfttruW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cell 12 â€” Notebook Wrap-Up\n",
        "# Purpose: Final message to wrap up the notebook for viewers and collaborators.\n",
        "# ==========================================================\n",
        "\n",
        "def notebook_wrap_up():\n",
        "    print(\"\"\"\n",
        "ðŸŽµ Musical Tracks Database â€” Notebook Complete!\n",
        "\n",
        "âœ… Phase 1 (Core Build): Done\n",
        "   - Schema created\n",
        "   - Data ingested\n",
        "   - Assignment joins verified\n",
        "\n",
        "âœ… Phase 2 (Portfolio Enhancements): Done\n",
        "   - Analytics & insights\n",
        "   - Exported CSV summaries\n",
        "   - Auto-generated README.md\n",
        "\n",
        "Next steps (optional):\n",
        "- Add a visual schema or ERD\n",
        "- Extend analytics or add DJ-focused fields (BPM, Key)\n",
        "- Package this into a CLI tool or Python module\n",
        "\n",
        "Thank you for exploring this project.\n",
        "Feel free to fork, extend, or remix it for your own use cases! ðŸš€\n",
        "\"\"\")\n",
        "\n",
        "print(\"Wrap-up cell ready. Call notebook_wrap_up().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxFuXPEhtsmp",
        "outputId": "307dca39-40e5-433c-e46b-99bb867fade0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrap-up cell ready. Call notebook_wrap_up().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 13 â€” Completion & Closing Message"
      ],
      "metadata": {
        "id": "IdJTllxduUVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_wrap_up()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSc3xt_buFiy",
        "outputId": "8d7bafed-bd90-4f0d-9f6a-b43a182b87b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽµ Musical Tracks Database â€” Notebook Complete!\n",
            "\n",
            "âœ… Phase 1 (Core Build): Done  \n",
            "   - Schema created  \n",
            "   - Data ingested  \n",
            "   - Assignment joins verified\n",
            "\n",
            "âœ… Phase 2 (Portfolio Enhancements): Done  \n",
            "   - Analytics & insights  \n",
            "   - Exported CSV summaries  \n",
            "   - Auto-generated README.md  \n",
            "\n",
            "Next steps (optional):\n",
            "- Add a visual schema or ERD\n",
            "- Extend analytics or add DJ-focused fields (BPM, Key)\n",
            "- Package this into a CLI tool or Python module\n",
            "\n",
            "Thank you for exploring this project.\n",
            "Feel free to fork, extend, or remix it for your own use cases! ðŸš€\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
